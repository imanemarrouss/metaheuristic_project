{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10239648,"sourceType":"datasetVersion","datasetId":6332174},{"sourceId":10253996,"sourceType":"datasetVersion","datasetId":6342815},{"sourceId":10323919,"sourceType":"datasetVersion","datasetId":6392012},{"sourceId":10351405,"sourceType":"datasetVersion","datasetId":6409956},{"sourceId":104492,"sourceType":"modelInstanceVersion","modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"-B8GLisnL7DI"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"id":"Ttqa4BruL7DL"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.model_download(\"google/gemma-2/transformers/gemma-2-9b\")\n\nprint(\"Path to model files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-20T17:48:48.465442Z","iopub.execute_input":"2024-12-20T17:48:48.465802Z","iopub.status.idle":"2024-12-20T17:48:49.394189Z","shell.execute_reply.started":"2024-12-20T17:48:48.465767Z","shell.execute_reply":"2024-12-20T17:48:49.393167Z"},"id":"9fauO3fCL7DL","outputId":"299164c0-c85c-47e5-ba36-a40f9cef07f7"},"outputs":[{"name":"stdout","text":"Path to model files: /kaggle/input/gemma-2/transformers/gemma-2-9b/2\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"\"\"\"Evaluation metric for Santa 2024.\"\"\"\n\nimport gc\nimport os\nfrom math import exp\nfrom collections import Counter\nfrom typing import List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport transformers\nimport torch\n\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'\nPAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass ParticipantVisibleError(Exception):\n    pass\n\n\ndef score(\n    solution: pd.DataFrame,\n    submission: pd.DataFrame,\n    row_id_column_name: str,\n    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n    load_in_8bit: bool = False,\n    clear_mem: bool = False,\n) -> float:\n    \"\"\"\n    Calculates the mean perplexity of submitted text permutations compared to an original text.\n\n    Parameters\n    ----------\n    solution : DataFrame\n        DataFrame containing the original text in a column named 'text'.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    submission : DataFrame\n        DataFrame containing the permuted text in a column named 'text'.\n        Must have the same row IDs as the solution.\n        Includes a row ID column specified by `row_id_column_name`.\n\n    row_id_column_name : str\n        Name of the column containing row IDs.\n        Ensures aligned comparison between solution and submission.\n\n    model_path : str, default='/kaggle/input/gemma-2/transformers/gemma-2-9b/2'\n        Path to the serialized LLM.\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    clear_mem : bool, default=False\n        Clear GPU memory after scoring by clearing the CUDA cache.\n        Useful for testing.\n\n    Returns\n    -------\n    float\n        The mean perplexity score. Lower is better.\n\n    Raises\n    ------\n    ParticipantVisibleError\n        If the submission format is invalid or submitted strings are not valid permutations.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n    >>> solution = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n    ... })\n    >>> submission = pd.DataFrame({\n    ...     'id': [0, 1],\n    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n    ... })\n    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n    True\n    \"\"\"\n    # Check that each submitted string is a permutation of the solution string\n    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n    invalid_mask = sol_counts != sub_counts\n    if invalid_mask.any():\n        raise ParticipantVisibleError(\n            'At least one submitted string is not a valid permutation of the solution string.'\n        )\n\n    # Calculate perplexity for the submitted strings\n    sub_strings = [\n        ' '.join(s.split()) for s in submission['text'].tolist()\n    ]  # Split and rejoin to normalize whitespace\n    scorer = PerplexityCalculator(\n    model_path='/kaggle/input/gemma-2/transformers/gemma-2-9b/2', \n    load_in_8bit=False\n)\n # Initialize the perplexity calculator with a pre-trained model\n    perplexities = scorer.get_perplexity(\n        sub_strings\n    )  # Calculate perplexity for each submitted string\n\n    if clear_mem:\n        # Just move on if it fails. Not essential if we have the score.\n        try:\n            scorer.clear_gpu_memory()\n        except:\n            print('GPU memory clearing failed.')\n\n    return float(np.mean(perplexities))\n\n\nclass PerplexityCalculator:\n    \"\"\"\n    Calculates perplexity of text using a pre-trained language model.\n\n    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n\n    Parameters\n    ----------\n    model_path : str\n        Path to the pre-trained language model\n\n    load_in_8bit : bool, default=False\n        Use 8-bit quantization for the model. Requires CUDA.\n\n    device_map : str, default=\"auto\"\n        Device mapping for the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str,\n        load_in_8bit: bool = False,\n        device_map: str = 'auto',\n    ):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n        # Configure model loading based on quantization setting and device availability\n        if load_in_8bit:\n            if DEVICE.type != 'cuda':\n                raise ValueError('8-bit quantization requires CUDA device')\n            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                quantization_config=quantization_config,\n                device_map=device_map,\n            )\n        else:\n            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n                model_path,\n                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n                device_map=device_map,\n            )\n\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n\n        self.model.eval()\n\n    def get_perplexity(self, input_texts: Union[str, List[str]], batch_size: int = 8, debug=False) -> Union[float, List[float]]:\n        \"\"\"\n        Optimized perplexity calculation using batching for efficiency.\n        \"\"\"\n        single_input = isinstance(input_texts, str)\n        input_texts = [input_texts] if single_input else input_texts\n        loss_list = []\n        \n        for i in range(0, len(input_texts), batch_size):\n            batch_texts = input_texts[i:i + batch_size]\n            with torch.no_grad():\n                model_inputs = self.tokenizer(\n                        batch_texts,\n                        return_tensors='pt',\n                        padding=True,\n                        truncation=True,\n                        max_length=512,  # Limit input sequence length\n                        add_special_tokens=True,\n                    ).to(DEVICE)\n\n                \n                \n                output = self.model(**model_inputs)\n                logits = output['logits']\n                \n                shift_logits = logits[..., :-1, :].contiguous()\n                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()\n                \n                loss = self.loss_fct(\n                    shift_logits.view(-1, shift_logits.size(-1)),\n                    shift_labels.view(-1)\n                )\n                \n                batch_losses = loss.view(len(batch_texts), -1).sum(dim=1) / model_inputs['attention_mask'].sum(dim=1)\n                loss_list.extend(batch_losses.cpu().tolist())\n                \n                if debug:\n                    print(f\"Processed batch {i // batch_size + 1} of {len(input_texts) // batch_size + 1}\")\n    \n        ppl = [exp(i) for i in loss_list]\n        return ppl[0] if single_input else ppl\n\n    def clear_gpu_memory(self) -> None:\n        \"\"\"Enhanced memory clearing.\"\"\"\n        if torch.cuda.is_available():\n            del self.model\n            del self.tokenizer\n            gc.collect()\n            with torch.cuda.device(DEVICE):\n                torch.cuda.empty_cache()\n                torch.cuda.ipc_collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset with perplexity 390\ninput_file = \"/kaggle/input/dataset39/submission (5).csv\"\ndata = pd.read_csv(input_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-01T23:46:35.972570Z","iopub.execute_input":"2025-01-01T23:46:35.972862Z","iopub.status.idle":"2025-01-01T23:46:35.978012Z","shell.execute_reply.started":"2025-01-01T23:46:35.972837Z","shell.execute_reply":"2025-01-01T23:46:35.977305Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport random\nimport numpy as np\n\nclass SimulatedAnnealing:\n    def __init__(self, initial_temperature, cooling_rate, max_iterations, random_state=42):\n        random.seed(random_state)\n        np.random.seed(random_state)\n        self.initial_temperature = initial_temperature\n        self.cooling_rate = cooling_rate\n        self.max_iterations = max_iterations\n\n    def swap(self, sequence):\n        \"\"\"Enhanced neighbor generation: Reversals and adaptive swaps.\"\"\"\n        neighbor = sequence[:]\n        if random.random() < 0.5:  # 50% chance to reverse a segment\n            i, j = sorted(random.sample(range(len(neighbor)), 2))\n            neighbor[i:j] = reversed(neighbor[i:j])\n        else:  # Otherwise, swap multiple pairs\n            num_swaps = random.randint(2, max(5, int(len(neighbor) * 0.1)))\n            for _ in range(num_swaps):\n                i, j = random.sample(range(len(neighbor)), 2)\n                neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n        return neighbor\n\n    def solve(self, text, scorer):\n        words = text.split()\n        current_sequence = words[:]\n        best_sequence = words[:]\n        current_perplexity = scorer.get_perplexity(' '.join(current_sequence))\n        best_perplexity = current_perplexity\n        temperature = self.initial_temperature\n        log_energies = []\n\n        while temperature > 0.1:  # Lower final temperature threshold\n            for _ in range(self.max_iterations):\n                neighbor = self.swap(current_sequence)\n                neighbor_perplexity = scorer.get_perplexity(' '.join(neighbor))\n                delta = neighbor_perplexity - current_perplexity\n\n                if delta < 0 or random.random() < np.exp(-delta / temperature):\n                    current_sequence = neighbor\n                    current_perplexity = neighbor_perplexity\n\n                    if current_perplexity < best_perplexity:\n                        best_sequence = current_sequence\n                        best_perplexity = current_perplexity\n\n            log_energies.append(best_perplexity)\n            print(f\"Temperature: {temperature:.2f}, Best Perplexity: {best_perplexity}\")\n            temperature *= self.cooling_rate\n\n        return ' '.join(best_sequence), best_perplexity\n\n            \n\ndef optimize_sequences_with_sa(input_file, output_file):\n    \"\"\"\n    Optimize text sequences using Simulated Annealing and save results in a CSV.\n\n    Parameters:\n    - input_file (str): Path to the input CSV file with 'id' and 'text'.\n    - output_file (str): Path to save the output CSV file.\n    \"\"\"\n    # Load the dataset\n    data = pd.read_csv(input_file)\n    results = []\n\n    # Initialize Simulated Annealing with specific parameters\n    sa = SimulatedAnnealing(\n        initial_temperature=50.0,  # Try a higher initial temperature\n        cooling_rate=0.98,  # Slower cooling\n        max_iterations=100,  # Increase the iterations\n        random_state=42\n    )\n\n    # Initialize the scorer\n    scorer = PerplexityCalculator(\n        model_path='/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n        load_in_8bit=False\n    )\n\n    # Optimize each text sequence\n    for idx, row in data.iterrows():\n        print(f\"Processing ID: {row['id']}\")\n        optimized_text, best_perplexity = sa.solve(row['text'], scorer)\n        results.append({'id': row['id'], 'text': optimized_text})\n\n    # Save the results to a CSV file\n    submission = pd.DataFrame(results)\n    submission.to_csv(output_file, index=False)\n    print(f\"Results saved to {output_file}\")\n\nif __name__ == \"__main__\":\n    input_file = \"/kaggle/input/dataset39/submission (5).csv\"\n    output_file = \"/kaggle/working/submission_refined.csv\"\n    optimize_sequences_with_sa(input_file, output_file)\n\n\n\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-02T00:07:41.322888Z","iopub.execute_input":"2025-01-02T00:07:41.323312Z","execution_failed":"2025-01-02T07:25:12.666Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5971c10f6184dffb0b55ef4c7b10e4e"}},"metadata":{}},{"name":"stdout","text":"Processing ID: 0\nTemperature: 50.00, Best Perplexity: 292.68395088034254\nTemperature: 49.00, Best Perplexity: 292.68395088034254\nTemperature: 48.02, Best Perplexity: 292.68395088034254\nTemperature: 47.06, Best Perplexity: 292.68395088034254\nTemperature: 46.12, Best Perplexity: 292.68395088034254\nTemperature: 45.20, Best Perplexity: 292.68395088034254\nTemperature: 44.29, Best Perplexity: 292.68395088034254\nTemperature: 43.41, Best Perplexity: 292.68395088034254\nTemperature: 42.54, Best Perplexity: 292.68395088034254\nTemperature: 41.69, Best Perplexity: 292.68395088034254\nTemperature: 40.85, Best Perplexity: 292.68395088034254\nTemperature: 40.04, Best Perplexity: 292.68395088034254\nTemperature: 39.24, Best Perplexity: 292.68395088034254\nTemperature: 38.45, Best Perplexity: 292.68395088034254\nTemperature: 37.68, Best Perplexity: 292.68395088034254\nTemperature: 36.93, Best Perplexity: 292.68395088034254\nTemperature: 36.19, Best Perplexity: 292.68395088034254\nTemperature: 35.47, Best Perplexity: 292.68395088034254\nTemperature: 34.76, Best Perplexity: 292.68395088034254\nTemperature: 34.06, Best Perplexity: 292.68395088034254\nTemperature: 33.38, Best Perplexity: 292.68395088034254\nTemperature: 32.71, Best Perplexity: 292.68395088034254\nTemperature: 32.06, Best Perplexity: 292.68395088034254\nTemperature: 31.42, Best Perplexity: 292.68395088034254\nTemperature: 30.79, Best Perplexity: 292.68395088034254\nTemperature: 30.17, Best Perplexity: 292.68395088034254\nTemperature: 29.57, Best Perplexity: 292.68395088034254\nTemperature: 28.98, Best Perplexity: 292.68395088034254\nTemperature: 28.40, Best Perplexity: 292.68395088034254\nTemperature: 27.83, Best Perplexity: 292.68395088034254\nTemperature: 27.27, Best Perplexity: 292.68395088034254\nTemperature: 26.73, Best Perplexity: 292.68395088034254\nTemperature: 26.19, Best Perplexity: 292.68395088034254\nTemperature: 25.67, Best Perplexity: 292.68395088034254\nTemperature: 25.16, Best Perplexity: 292.68395088034254\nTemperature: 24.65, Best Perplexity: 292.68395088034254\nTemperature: 24.16, Best Perplexity: 292.68395088034254\nTemperature: 23.68, Best Perplexity: 292.68395088034254\nTemperature: 23.20, Best Perplexity: 292.68395088034254\nTemperature: 22.74, Best Perplexity: 292.68395088034254\nTemperature: 22.29, Best Perplexity: 292.68395088034254\nTemperature: 21.84, Best Perplexity: 292.68395088034254\nTemperature: 21.40, Best Perplexity: 292.68395088034254\nTemperature: 20.97, Best Perplexity: 292.68395088034254\nTemperature: 20.55, Best Perplexity: 292.68395088034254\nTemperature: 20.14, Best Perplexity: 292.68395088034254\nTemperature: 19.74, Best Perplexity: 292.68395088034254\nTemperature: 19.35, Best Perplexity: 292.68395088034254\nTemperature: 18.96, Best Perplexity: 292.68395088034254\nTemperature: 18.58, Best Perplexity: 292.68395088034254\nTemperature: 18.21, Best Perplexity: 292.68395088034254\nTemperature: 17.84, Best Perplexity: 292.68395088034254\nTemperature: 17.49, Best Perplexity: 292.68395088034254\nTemperature: 17.14, Best Perplexity: 292.68395088034254\nTemperature: 16.79, Best Perplexity: 292.68395088034254\nTemperature: 16.46, Best Perplexity: 292.68395088034254\nTemperature: 16.13, Best Perplexity: 292.68395088034254\nTemperature: 15.81, Best Perplexity: 292.68395088034254\nTemperature: 15.49, Best Perplexity: 292.68395088034254\nTemperature: 15.18, Best Perplexity: 292.68395088034254\nTemperature: 14.88, Best Perplexity: 292.68395088034254\nTemperature: 14.58, Best Perplexity: 292.68395088034254\nTemperature: 14.29, Best Perplexity: 292.68395088034254\nTemperature: 14.00, Best Perplexity: 292.68395088034254\nTemperature: 13.72, Best Perplexity: 292.68395088034254\nTemperature: 13.45, Best Perplexity: 292.68395088034254\nTemperature: 13.18, Best Perplexity: 292.68395088034254\nTemperature: 12.92, Best Perplexity: 292.68395088034254\nTemperature: 12.66, Best Perplexity: 292.68395088034254\nTemperature: 12.40, Best Perplexity: 292.68395088034254\nTemperature: 12.16, Best Perplexity: 292.68395088034254\nTemperature: 11.91, Best Perplexity: 292.68395088034254\nTemperature: 11.67, Best Perplexity: 292.68395088034254\nTemperature: 11.44, Best Perplexity: 292.68395088034254\nTemperature: 11.21, Best Perplexity: 292.68395088034254\nTemperature: 10.99, Best Perplexity: 292.68395088034254\nTemperature: 10.77, Best Perplexity: 292.68395088034254\nTemperature: 10.55, Best Perplexity: 292.68395088034254\nTemperature: 10.34, Best Perplexity: 292.68395088034254\nTemperature: 10.14, Best Perplexity: 292.68395088034254\nTemperature: 9.93, Best Perplexity: 292.68395088034254\nTemperature: 9.73, Best Perplexity: 292.68395088034254\nTemperature: 9.54, Best Perplexity: 292.68395088034254\nTemperature: 9.35, Best Perplexity: 292.68395088034254\nTemperature: 9.16, Best Perplexity: 292.68395088034254\nTemperature: 8.98, Best Perplexity: 292.68395088034254\nTemperature: 8.80, Best Perplexity: 292.68395088034254\nTemperature: 8.62, Best Perplexity: 292.68395088034254\nTemperature: 8.45, Best Perplexity: 292.68395088034254\nTemperature: 8.28, Best Perplexity: 292.68395088034254\nTemperature: 8.12, Best Perplexity: 292.68395088034254\nTemperature: 7.95, Best Perplexity: 292.68395088034254\nTemperature: 7.79, Best Perplexity: 292.68395088034254\nTemperature: 7.64, Best Perplexity: 292.68395088034254\nTemperature: 7.49, Best Perplexity: 292.68395088034254\nTemperature: 7.34, Best Perplexity: 292.68395088034254\nTemperature: 7.19, Best Perplexity: 292.68395088034254\nTemperature: 7.05, Best Perplexity: 292.68395088034254\nTemperature: 6.90, Best Perplexity: 292.68395088034254\nTemperature: 6.77, Best Perplexity: 292.68395088034254\nTemperature: 6.63, Best Perplexity: 292.68395088034254\nTemperature: 6.50, Best Perplexity: 292.68395088034254\nTemperature: 6.37, Best Perplexity: 292.68395088034254\nTemperature: 6.24, Best Perplexity: 292.68395088034254\nTemperature: 6.12, Best Perplexity: 292.68395088034254\nTemperature: 5.99, Best Perplexity: 292.68395088034254\nTemperature: 5.87, Best Perplexity: 292.68395088034254\nTemperature: 5.76, Best Perplexity: 292.68395088034254\nTemperature: 5.64, Best Perplexity: 292.68395088034254\nTemperature: 5.53, Best Perplexity: 292.68395088034254\nTemperature: 5.42, Best Perplexity: 292.68395088034254\nTemperature: 5.31, Best Perplexity: 292.68395088034254\nTemperature: 5.20, Best Perplexity: 292.68395088034254\nTemperature: 5.10, Best Perplexity: 292.68395088034254\nTemperature: 5.00, Best Perplexity: 292.68395088034254\nTemperature: 4.90, Best Perplexity: 292.68395088034254\nTemperature: 4.80, Best Perplexity: 292.68395088034254\nTemperature: 4.70, Best Perplexity: 292.68395088034254\nTemperature: 4.61, Best Perplexity: 292.68395088034254\nTemperature: 4.52, Best Perplexity: 292.68395088034254\nTemperature: 4.43, Best Perplexity: 292.68395088034254\nTemperature: 4.34, Best Perplexity: 292.68395088034254\nTemperature: 4.25, Best Perplexity: 292.68395088034254\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import pandas as pd  \nimport numpy as np   \nimport random        \nimport math          \nfrom copy import deepcopy  \n\n# Définition de la classe Ant (Fourmi)\nclass Ant:\n    def __init__(self, sequence, scorer):\n        # Initialisation de la fourmi avec une séquence de mots et un évaluateur (scorer)\n        self.sequence = sequence[:]  # Séquence actuelle de la fourmi\n        self.best_sequence = sequence[:]  # Meilleure séquence trouvée par la fourmi\n        self.best_perplexity = float('inf')  # Meilleure perplexité (initialisée à l'infini)\n        self.scorer = scorer  # Évaluateur pour calculer la perplexité\n        self.evaluate()  # Évaluation initiale de la séquence\n\n    def evaluate(self):\n        \"\"\"Évalue la séquence et met à jour la meilleure solution.\"\"\"\n        self.current_perplexity = self.scorer.get_perplexity(' '.join(self.sequence))  # Calcule la perplexité\n        if self.current_perplexity < self.best_perplexity:  # Si la perplexité est meilleure\n            self.best_perplexity = self.current_perplexity  # Met à jour la meilleure perplexité\n            self.best_sequence = self.sequence[:]  # Met à jour la meilleure séquence\n\n# Définition de la fonction de Recuit Simulé (Simulated Annealing)\ndef simulated_annealing(sequence, scorer, initial_temp=150, cooling_rate=0.95, max_iterations=20):\n    \"\"\"Recuit Simulé pour la recherche locale.\"\"\"\n    current_sequence = sequence[:]  # Séquence actuelle\n    best_sequence = sequence[:]  # Meilleure séquence trouvée\n    best_perplexity = scorer.get_perplexity(' '.join(best_sequence))  # Calcule la perplexité initiale\n    current_temp = initial_temp  # Température initiale\n\n    for _ in range(max_iterations):\n        # Génère un voisin en échangeant deux mots aléatoirement\n        neighbor = current_sequence[:]\n        i, j = random.sample(range(len(neighbor)), 2)\n        neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n\n        # Évalue le voisin\n        neighbor_perplexity = scorer.get_perplexity(' '.join(neighbor))\n\n        # Accepte ou rejette le voisin en fonction de la perplexité et de la température\n        delta = neighbor_perplexity - best_perplexity\n        if delta < 0 or random.random() < math.exp(-delta / current_temp):\n            current_sequence = neighbor[:]\n            if neighbor_perplexity < best_perplexity:\n                best_sequence = neighbor[:]\n                best_perplexity = neighbor_perplexity\n\n        current_temp *= cooling_rate  # Refroidissement de la température\n\n    return best_sequence, best_perplexity  # Retourne la meilleure séquence et sa perplexité\n\n# Définition de la fonction ACO combinée avec le Recuit Simulé\ndef aco_with_sa(text, scorer, n_ants=25, max_iterations=20, alpha=1.0,\n                beta=2.0, evaporation_rate=0.5, pheromone_init=1.0):\n    \"\"\"ACO combiné avec SA pour l'optimisation de séquences.\"\"\"\n    # Divise le texte en mots\n    words = text.split()\n    # Nombre de mots dans le texte\n    n_words = len(words)\n\n    # Initialisation de la matrice de phéromones\n    # Chaque transition entre deux mots a une quantité initiale de phéromones (pheromone_init)\n    pheromones = np.full((n_words, n_words), pheromone_init)\n\n    # Initialisation de la meilleure solution globale\n    # Commence avec la séquence originale des mots\n    global_best_sequence = words[:]\n    # Calcule la perplexité de la séquence originale\n    global_best_perplexity = scorer.get_perplexity(' '.join(global_best_sequence))\n\n    # Boucle principale sur le nombre d'itérations\n    for iteration in range(max_iterations):\n        print(f\"Iteration {iteration + 1}/{max_iterations}, Best perplexity: {global_best_perplexity}\")\n\n        # Liste pour stocker les fourmis\n        ants = []\n        # Crée une fourmi pour chaque fourmi dans la colonie\n        for _ in range(n_ants):\n            # Construction d'une solution pour chaque fourmi\n            sequence = []\n            # Liste des mots disponibles pour la construction de la séquence\n            available_words = words[:]\n            for _ in range(n_words):\n                probabilities = []\n                # Calcule les probabilités de transition pour chaque mot disponible\n                for word in available_words:\n                    word_idx = words.index(word)\n                    # Probabilité basée sur les phéromones et l'heuristique (alpha et beta)\n                    probabilities.append(pheromones[len(sequence), word_idx] ** alpha)\n                # Normalisation des probabilités pour qu'elles somment à 1\n                probabilities = np.array(probabilities)\n                probabilities /= probabilities.sum()\n                # Choix d'un mot en fonction des probabilités\n                chosen_word = random.choices(available_words, weights=probabilities, k=1)[0]\n                # Ajoute le mot choisi à la séquence\n                sequence.append(chosen_word)\n                # Retire le mot choisi des mots disponibles\n                available_words.remove(chosen_word)\n\n            # Crée une fourmi avec la séquence construite et l'évaluateur (scorer)\n            ant = Ant(sequence, scorer)\n            # Ajoute la fourmi à la liste des fourmis\n            ants.append(ant)\n\n        # Recherche locale avec Recuit Simulé pour chaque fourmi\n        for ant in ants:\n            # Applique le Recuit Simulé pour améliorer la séquence de la fourmi\n            improved_sequence, improved_perplexity = simulated_annealing(\n                ant.sequence,\n                scorer,\n                initial_temp=150,\n                cooling_rate=0.98,\n                max_iterations=10\n            )\n            # Si la perplexité améliorée est meilleure, met à jour la séquence de la fourmi\n            if improved_perplexity < ant.best_perplexity:\n                ant.sequence = improved_sequence[:]\n                ant.best_perplexity = improved_perplexity\n\n            # Met à jour la meilleure solution globale si la fourmi a trouvé une meilleure séquence\n            if ant.best_perplexity < global_best_perplexity:\n                global_best_sequence = ant.sequence[:]\n                global_best_perplexity = ant.best_perplexity\n\n        # Mise à jour des phéromones\n        # Évaporation des phéromones : réduit toutes les phéromones par un facteur (1 - evaporation_rate)\n        pheromones *= (1 - evaporation_rate)\n        # Ajout de phéromones sur les chemins utilisés par les fourmis\n        for ant in ants:\n            for i in range(len(ant.sequence) - 1):\n                # Indices des mots dans la séquence\n                word_idx1 = words.index(ant.sequence[i])\n                word_idx2 = words.index(ant.sequence[i + 1])\n                # Ajoute des phéromones proportionnellement à la qualité de la séquence (1 / perplexité)\n                pheromones[word_idx1, word_idx2] += 1.0 / ant.best_perplexity\n\n    # Retourne la meilleure séquence globale et sa perplexité\n    return global_best_sequence, global_best_perplexity\n    \n# Définition de la fonction pour optimiser les séquences par lots\ndef optimize_sequences(batch_size=5):\n    \"\"\"Optimise les séquences en utilisant ACO avec SA.\"\"\"\n    sample_submission = pd.read_csv(\"/kaggle/input/dataset/sample_submission.csv\")  # Charge les données\n    results = []  # Pour stocker les résultats\n    perplexity_scores = []  # Pour stocker les scores de perplexité\n\n    # Divise les données en lots\n    num_batches = (len(sample_submission) + batch_size - 1) // batch_size\n\n    for batch_idx in range(num_batches):\n        batch_start = batch_idx * batch_size\n        batch_end = min((batch_idx + 1) * batch_size, len(sample_submission))\n        batch_data = sample_submission.iloc[batch_start:batch_end]\n\n        print(f\"\\nProcessing batch {batch_idx + 1}/{num_batches}\")\n\n        try:\n            # Initialisation de l'évaluateur de perplexité\n            scorer = PerplexityCalculator(\n                model_path='/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n                load_in_8bit=False\n            )\n\n            # Traitement de chaque ligne dans le lot\n            for idx, row in batch_data.iterrows():\n                optimized_text, perplexity = aco_with_sa(\n                    row['text'],\n                    scorer,\n                    n_ants=45,  \n                    max_iterations=20,  \n                    alpha=1.0,\n                    beta=2.0,\n                    evaporation_rate=0.5,\n                    pheromone_init=1.0\n                )\n                print(f\"ID: {row['id']}, Final Perplexity: {perplexity}\")\n                results.append({'id': row['id'], 'text': optimized_text})  # Ajoute le résultat\n                perplexity_scores.append(perplexity)  # Ajoute la perplexité\n\n            # Sauvegarde intermédiaire des résultats\n            temp_df = pd.DataFrame(results)\n            temp_df.to_csv(f\"submission_temp_batch_{batch_idx+1}.csv\", index=False)\n\n        except Exception as e:\n            print(f\"Error processing batch {batch_idx + 1}: {str(e)}\")\n            # Gestion des erreurs : sauvegarde la séquence originale en cas d'échec\n            for idx, row in batch_data.iterrows():\n                results.append({'id': row['id'], 'text': row['text']})\n                perplexity_scores.append(float('inf'))  # Utilise l'infini pour les séquences échouées\n\n    # Sauvegarde finale des résultats\n    submission = pd.DataFrame(results)\n    submission.to_csv(\"/kaggle/working/submission.csv\", index=False)\n\n    # Calcul et affichage des métriques\n    perplexity_scores = np.array(perplexity_scores)\n    lowest_perplexity = np.min(perplexity_scores)  # Perplexité minimale\n    average_perplexity = np.mean(perplexity_scores)  # Perplexité moyenne\n\n    print(\"\\nFinal Results:\")\n    print(f\"Lowest Perplexity: {lowest_perplexity}\")\n    print(f\"Average Perplexity: {average_perplexity}\")\n\n    return submission  # Retourne les résultats finaux\n\n# Point d'entrée du programme\nif __name__ == \"__main__\":\n    print(\"Starting optimization...\")\n    final_submission = optimize_sequences()  # Lance l'optimisation\n    print(\"Optimization completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T02:19:58.141820Z","iopub.execute_input":"2025-01-03T02:19:58.142394Z","iopub.status.idle":"2025-01-03T09:20:36.089627Z","shell.execute_reply.started":"2025-01-03T02:19:58.142362Z","shell.execute_reply":"2025-01-03T09:20:36.088763Z"}},"outputs":[{"name":"stdout","text":"Starting optimization...\n\nProcessing batch 1/2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f356720749a4ecb9ec05db9ae037854"}},"metadata":{}},{"name":"stdout","text":"Iteration 1/20, Best perplexity: 2349.3494739881\nIteration 2/20, Best perplexity: 440.42301065338194\nIteration 3/20, Best perplexity: 440.42301065338194\nIteration 4/20, Best perplexity: 404.5354733638809\nIteration 5/20, Best perplexity: 361.2114152344687\nIteration 6/20, Best perplexity: 361.2114152344687\nIteration 7/20, Best perplexity: 361.2114152344687\nIteration 8/20, Best perplexity: 361.2114152344687\nIteration 9/20, Best perplexity: 361.2114152344687\nIteration 10/20, Best perplexity: 361.2114152344687\nIteration 11/20, Best perplexity: 361.2114152344687\nIteration 12/20, Best perplexity: 361.2114152344687\nIteration 13/20, Best perplexity: 361.2114152344687\nIteration 14/20, Best perplexity: 361.2114152344687\nIteration 15/20, Best perplexity: 361.2114152344687\nIteration 16/20, Best perplexity: 361.2114152344687\nIteration 17/20, Best perplexity: 336.45752274778374\nIteration 18/20, Best perplexity: 336.45752274778374\nIteration 19/20, Best perplexity: 336.45752274778374\nIteration 20/20, Best perplexity: 336.45752274778374\nID: 0, Final Perplexity: 336.45752274778374\nIteration 1/20, Best perplexity: 4393.407048674041\nIteration 2/20, Best perplexity: 1412.066515920572\nIteration 3/20, Best perplexity: 1361.2249477664523\nIteration 4/20, Best perplexity: 1361.2249477664523\nIteration 5/20, Best perplexity: 1361.2249477664523\nIteration 6/20, Best perplexity: 1361.2249477664523\nIteration 7/20, Best perplexity: 1361.2249477664523\nIteration 8/20, Best perplexity: 1361.2249477664523\nIteration 9/20, Best perplexity: 1361.2249477664523\nIteration 10/20, Best perplexity: 1361.2249477664523\nIteration 11/20, Best perplexity: 1050.4385916673102\nIteration 12/20, Best perplexity: 1050.4385916673102\nIteration 13/20, Best perplexity: 1050.4385916673102\nIteration 14/20, Best perplexity: 1050.4385916673102\nIteration 15/20, Best perplexity: 1050.4385916673102\nIteration 16/20, Best perplexity: 1050.4385916673102\nIteration 17/20, Best perplexity: 1050.4385916673102\nIteration 18/20, Best perplexity: 1050.4385916673102\nIteration 19/20, Best perplexity: 1028.3162194867045\nIteration 20/20, Best perplexity: 1028.3162194867045\nID: 1, Final Perplexity: 1028.3162194867045\nIteration 1/20, Best perplexity: 861.3509308083746\nIteration 2/20, Best perplexity: 818.4944387323065\nIteration 3/20, Best perplexity: 667.5204855749937\nIteration 4/20, Best perplexity: 511.14866656281464\nIteration 5/20, Best perplexity: 511.14866656281464\nIteration 6/20, Best perplexity: 511.14866656281464\nIteration 7/20, Best perplexity: 511.14866656281464\nIteration 8/20, Best perplexity: 511.14866656281464\nIteration 9/20, Best perplexity: 511.14866656281464\nIteration 10/20, Best perplexity: 511.14866656281464\nIteration 11/20, Best perplexity: 511.14866656281464\nIteration 12/20, Best perplexity: 511.14866656281464\nIteration 13/20, Best perplexity: 511.14866656281464\nIteration 14/20, Best perplexity: 511.14866656281464\nIteration 15/20, Best perplexity: 511.14866656281464\nIteration 16/20, Best perplexity: 511.14866656281464\nIteration 17/20, Best perplexity: 511.14866656281464\nIteration 18/20, Best perplexity: 511.14866656281464\nIteration 19/20, Best perplexity: 511.14866656281464\nIteration 20/20, Best perplexity: 511.14866656281464\nID: 2, Final Perplexity: 511.14866656281464\nIteration 1/20, Best perplexity: 1078.957248778673\nIteration 2/20, Best perplexity: 912.5193733079313\nIteration 3/20, Best perplexity: 912.5193733079313\nIteration 4/20, Best perplexity: 912.5193733079313\nIteration 5/20, Best perplexity: 906.6312206568098\nIteration 6/20, Best perplexity: 906.6312206568098\nIteration 7/20, Best perplexity: 906.6312206568098\nIteration 8/20, Best perplexity: 906.6312206568098\nIteration 9/20, Best perplexity: 906.6312206568098\nIteration 10/20, Best perplexity: 906.6312206568098\nIteration 11/20, Best perplexity: 906.6312206568098\nIteration 12/20, Best perplexity: 906.6312206568098\nIteration 13/20, Best perplexity: 906.6312206568098\nIteration 14/20, Best perplexity: 906.6312206568098\nIteration 15/20, Best perplexity: 906.6312206568098\nIteration 16/20, Best perplexity: 906.6312206568098\nIteration 17/20, Best perplexity: 906.6312206568098\nIteration 18/20, Best perplexity: 906.6312206568098\nIteration 19/20, Best perplexity: 906.6312206568098\nIteration 20/20, Best perplexity: 906.6312206568098\nID: 3, Final Perplexity: 906.6312206568098\nIteration 1/20, Best perplexity: 329.05520985317116\nIteration 2/20, Best perplexity: 329.05520985317116\nIteration 3/20, Best perplexity: 329.05520985317116\nIteration 4/20, Best perplexity: 329.05520985317116\nIteration 5/20, Best perplexity: 329.05520985317116\nIteration 6/20, Best perplexity: 329.05520985317116\nIteration 7/20, Best perplexity: 329.05520985317116\nIteration 8/20, Best perplexity: 329.05520985317116\nIteration 9/20, Best perplexity: 329.05520985317116\nIteration 10/20, Best perplexity: 329.05520985317116\nIteration 11/20, Best perplexity: 329.05520985317116\nIteration 12/20, Best perplexity: 329.05520985317116\nIteration 13/20, Best perplexity: 329.05520985317116\nIteration 14/20, Best perplexity: 329.05520985317116\nIteration 15/20, Best perplexity: 329.05520985317116\nIteration 16/20, Best perplexity: 329.05520985317116\nIteration 17/20, Best perplexity: 329.05520985317116\nIteration 18/20, Best perplexity: 329.05520985317116\nIteration 19/20, Best perplexity: 329.05520985317116\nIteration 20/20, Best perplexity: 329.05520985317116\nID: 4, Final Perplexity: 329.05520985317116\n\nProcessing batch 2/2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55a057646f0c4a33b0af6999224b7812"}},"metadata":{}},{"name":"stdout","text":"Iteration 1/20, Best perplexity: 346.46759300228723\nIteration 2/20, Best perplexity: 346.46759300228723\nIteration 3/20, Best perplexity: 346.46759300228723\nIteration 4/20, Best perplexity: 346.46759300228723\nIteration 5/20, Best perplexity: 346.46759300228723\nIteration 6/20, Best perplexity: 346.46759300228723\nIteration 7/20, Best perplexity: 346.46759300228723\nIteration 8/20, Best perplexity: 346.46759300228723\nIteration 9/20, Best perplexity: 346.46759300228723\nIteration 10/20, Best perplexity: 346.46759300228723\nIteration 11/20, Best perplexity: 346.46759300228723\nIteration 12/20, Best perplexity: 346.46759300228723\nIteration 13/20, Best perplexity: 346.46759300228723\nIteration 14/20, Best perplexity: 346.46759300228723\nIteration 15/20, Best perplexity: 346.46759300228723\nIteration 16/20, Best perplexity: 346.46759300228723\nIteration 17/20, Best perplexity: 346.46759300228723\nIteration 18/20, Best perplexity: 346.46759300228723\nIteration 19/20, Best perplexity: 346.46759300228723\nIteration 20/20, Best perplexity: 346.46759300228723\nID: 5, Final Perplexity: 346.46759300228723\n\nFinal Results:\nLowest Perplexity: 329.05520985317116\nAverage Perplexity: 576.3460720515951\nOptimization completed!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}